\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{optidef}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Vision-Based Pose Estimation for Launch Vehicle Booster Landing\\
}

\author{\IEEEauthorblockN{Anshuk Chigullapalli}
\IEEEauthorblockA{\textit{Aeronautics and Astronautics} \\
\textit{Stanford University}\\
Stanford, United States \\
anshuk@stanford.edu}
\and
\IEEEauthorblockN{Kristopher Riordan}
\IEEEauthorblockA{\textit{Aeronautics and Astronautics} \\
\textit{Stanford University}\\
Stanford, United States \\
riordk@stanford.edu}
\and
\IEEEauthorblockN{Yuji Takubo}
\IEEEauthorblockA{\textit{Aeronautics and Astronautics} \\
\textit{Stanford University}\\
Stanford, United States \\
ytakubo@stanford.edu}
\and
\IEEEauthorblockN{Faress Zwain}
\IEEEauthorblockA{\textit{Aeronautics and Astronautics} \\
\textit{Stanford University}\\
Stanford, United States \\
fzwain@stanford.edu}
}

\maketitle

\begin{abstract}
A vision-based attitude estimation algorithm for a rocket booster landing is developed. 
In a 3D-rendered world, an ArUcro Marker (similar to an AprilTag) placed at the launchpad is utilized as a key landmark of feature detection and matching. 
The measurement is then processed to update a state estimation using the multiplicative extended Kalman filter, where the rotational uncertainty is expressed as a covariance matrix of modified Rodrigues parameters. 
The developed algorithm is validated via a comparison with the IMU-based pose estimation as well as the combined measurement, and the performance of each estimation system is analyzed.  
\end{abstract}

% \begin{IEEEkeywords}
% Pose estimation, Terrain-Relative Navigation, 
% \end{IEEEkeywords}

\section{Introduction}

Reusable rocket boosters are an active point of development in the launch vehicle industry. 
Re-using rocket boosters (i.e. the first stage of a launch vehicle) can significantly reduce the cost of space launch services and increase launch cadence \cite{shotwell2021space}. 
Through these benefits, reusable rockets also make technologies such as in-orbit propellant transfer and long-duration Mars missions economically viable; reliable reusability is a cornerstone of space exploration.

A demonstrated method of retrieving orbital-class boosters is vertical landing with retro-propulsion. 
This has been demonstrated by SpaceX's Falcon 9 and Falcon Heavy (orbital booster) and Blue Origin's New Shepard (sub-orbital booster) \cite{maggio2023vision} \cite{blackmore2016autonomous}. 
As the industry works towards vertical landings with higher precision, such as landing the booster back at the launch pad, a stronger requirement on precise and robust navigation and state estimation algorithm during the final stages of this landing maneuver is desired \cite{blackmore2016autonomous}.

Vision-based estimation has already been demonstrated in space missions such as the Mars 2020 mission \cite{johnson2022mars} or IM-1 lunar lander \cite{christian2021image}, which utilized terrain relative navigation (TRN) for their position estimation and precise autonomous landing. 

This paper evaluates the effectiveness and applicability of the vision-based pose estimation to the booster landing problem. 
One differentiation of the booster landing compared to the planetary landing problem is that we have exact prior knowledge of the landing site and surroundings, while a sub-meter level of navigation accuracy is desired for the pin-point landing. 
In this paper, we further simplify the problem by placing an AprilTag \cite{kallwies2020determining} as a defining landmark for the landing site. 
The pose of the booster is determined by comparing existing onboard camera footage facing the landing pad and pointing to known features at the landing site.
We then process a vision-based pose estimation algorithm with generic IMU-based pose estimation using an Extended Kalman Filter (EKF) to get an improved estimate.  


\section{Related Work}

Image-based localization using cameras can be an effective replacement for time-based filters for highly dynamic systems can result in a loss of information or blur in the dynamics \cite{mair2009efficient}. 
Although a single calibrated camera may achieve an accurate localization, it is best for low-noise measurements with close-range applications.
Reference \cite{mair2009efficient} uses a modified Kanade-Lucas-Tomasi (KLT) tracker for feature tracking and a vision-based GPS for self-estimating the camera poses. 
The image scaling is first initialized using a known feature dimension, structure from motion (SfM), or a stereo camera system with triangulation; then the features are managed at each time step. The algorithm determines whether enough features are trackable to continue high-accuracy pose estimation; if not, the system loops back to KLT to re-initialize features for tracking. 
Experimental results and accuracy comparisons between different models are discussed in more detail \cite{mair2009efficient}.

AprilTags are square tags with distinct monochrome patterns that are commonly used for landmarks of feature detection and estimation algorithms. 
Detailed real-time pose estimation, tracking, and localization in GPS-denied environments and comparisons of the size, accuracy, and speed of several AprilTag implementation algorithms are discussed in Ref. \cite{tola2021real}. 
The high-level design includes grayscale, Gaussian smoothing, gradient magnitude \& direction, and edge detection \& clustering. 
Once a set of candidate quads (four-sided shapes made up of the detected edge segments) is generated, the camera extrinsics problem is solved to extract a rotation and translation of the camera in relation to a frame grounded in the AprilTag itself \cite{olson2011apriltag}. 

Due to the variety of different AprilTag detection libraries available, a research project was conducted to compare the localization accuracy of four commonly used, open-source AprilTag detection libraries \cite{kallwies2020determining}. 
Using a 3D camera simulation from OpenGL, the researchers tested each library's localization accuracy with variable parameters such as tag size, viewing angle, tag rotations, and border occlusion. The authors also explain different localization accuracy improvement methods including edge refinement, corner refinement, tag extensions, and filtering out poor tag detections. 
The conclusion states that ArUco OpenCV is best for computational speed, AprilTags C++ is best for accuracy, and AprilTags 3 is the best for a middle ground \cite{kallwies2020determining}. 

Similar, but opposite to our project, robot localization using images of a moving object has proven to be a successful localization method \cite{lee2003localization}. 
The two main methods are position estimation using image projection of the moving target and then correction through the use of a Kalman filter. 
The simulation and experiments show the Kalman filter's ability to recursively correct the position estimation after an initial estimate from the image projection \cite{lee2003localization}. 
Although the camera frame is fixed and the target is moving relative to the camera, the same algorithms are applicable if the camera is moving relative to a fixed target frame. 

In the case of having no AprilTags, utilizing a known map of generic terrain features becomes necessary. Camera-based TRN falls into a subset of visual odometry problems that estimate an ego-motion and pose by moving camera images \cite{nister2004visual, scaramuzza2011visual}, which is a variant of the structure from motion (SfM) problem.
Since it may not require any prior information about the landscape and the trajectory motion but estimates the agent's odometry purely based on iterative feature-matching and outlier rejections, successful real-world online implementations has been demonstrated \cite{christian2021image}.

The Kalman Filter and its variations are the most common sequential state estimation techniques. 
When using a Kalman Filter to estimate the attitude of a rigid body, the state space must be chosen carefully. 
Although quaternions are a typical attitude representation for spacecraft due to the lack of singularities, a unitary norm constraint ($\|q\|=1$) spawns difficulty in the standard Kalman Filter formulation \cite{markley2003mekf}. 
One solution for this issue is a multiplicative Kalman filter (MKF), which utilizes a state space that is composed of the error rotation between the belief state and an updated attitude. 
This error rotation can be chosen to be one of several unconstrained attitude parameterizations (e.g. Gibbs vector, Modified Rodrigues Parameters, Euler angles). If chosen well, this parameterization yields a simple, first-order formulation of the Kalman Filter \cite{markley2003mekf}.

The guidance problem during rocket descent involves the generation of valid trajectories subject to physically imposed and designer imposed constraints. This problem can be formulated as a convex optimization problem using lossless convexification to eliminate pervasive non-convex dynamics and constraints \cite{malyuta2021convex}. Additionally, assuming that the attitude control dynamics operate on a much faster time-scale than the translational dynamics, the trajectory can be simplified to that of a point mass with three degrees of freedom for which the thrust vector dictates the attitude. Using these simplifications and discretizing the dynamics allows for simple and fast trajectory generation that can be done on-board the descent vehicle, often with just a single call of a convex solver \cite{malyuta2021convex}. 

\section{Methodology}

We propose a fully integrated vision-based pose estimation package that takes in camera information and, utilizing existing map information of the landing site, determines the pose of the vehicle through feature detection and matching.

This will be implemented in a simulated environment that combines Blender, a popular 3D graphics tool, and Python. A pre-determined landing trajectory is generated using the methods described in Section \ref{sec:dynamics}. Then, we acquire a sequence of noised-images from a simulated camera in Blender that follows that pre-determined trajectory. These sequence of images are used in the pose-estimation problem. To simplify the problem, the pose estimates are not used in the loop with guidance and control problem. The vision-based pose estimate will then be fused with a simulated IMU pose estimate, to determine if the added visual odometry information improves the final result. Figure \ref{fig:flow_chart} showcases the information flow of the proposed work.

% insert figure here
\begin{figure}[htbp] \label{fig:flow_chart}
    \centerline{\includegraphics[width=0.49\textwidth]{273_project_flow.png}}
    \caption{Proposed vision-based estimation experiment architecture.}
    \label{fig:sys_arch}
\end{figure}
% insert figure here

\subsection{Vehicle Dynamics and Trajectory Generation} \label{sec:dynamics}

The translational trajectory of the rocket is generated using a 3-DOF model of the dynamics, treating the vehicle as a point mass. Imposed on the model are discrete state transition constraints, a pointing constraint, a glide-slope constraint, a pointing constraint, and initial and final conditions set. 
All non-convexities are handled using lossless convexification as described in Ref. \cite{malyuta2021convex}. 
Solving the problem using CVXPY yields a fuel optimal trajectory, which encodes a time history of the rocket's position in 3D space along with a thrust vector. 

The treatment of this problem in continuous time is discussed in detail in Ref. \cite{malyuta2021convex}.
Several constraints in the original formulation of the problem are either non-convex or require changes of variables in order to be DCP compliant.
This leads to what is denoted as Problem 104 in Ref. \cite{malyuta2021convex}, which is cast in continuous time. Discretizing this problem yields what is defined below as Problem \ref{eq:convex_traj_gen}.

\begin{mini!}|s|[2]<b>{\xi_0, \hdots , \xi_K}{\sum_{k=1}^{K}{\xi_k }\label{eq:objective}}{\label{eq:convex_traj_gen}}{}
\addConstraint{\vec{r}_{k+1}}{= \vec{r}_k + \frac{\delta t}{2} \left( \vec{v}_k + \vec{v}_{k+1} \right)}
\addConstraint{\vec{v}_{k+1}}{= \vec{v}_k + \delta t \left( \vec{u}_k - g\hat{e}_z \right)}
\addConstraint{z_{k+1}}{= z_k - \delta t \alpha \xi_k}
\addConstraint{\mu_{\text{min},k} \left( 1 - \delta z_k + \frac{1}{2} {\delta z_k}^2 \right)}{\leq \xi_k}
\addConstraint{\mu_{\text{max},k} \left( 1 - \delta z_k \right)}{\geq \xi_k}
\addConstraint{\Vert \vec{u}_k \Vert }{\leq \xi_k}
\addConstraint{\vec{u}_k \hat{e}_z^T}{\geq \xi_k \cos{\gamma_{p,k}}}
\addConstraint{\vec{r}_k^T \hat{e}_z}{\geq \gamma_{gs} \Vert H_{gs} \vec{r}_k \Vert}
\addConstraint{\ln{(m_{\text{dry}})}}{\leq z_K}
\addConstraint{z_1}{= \ln{(m_{\text{wet}})}}
\addConstraint{z_{l,k}}{\leq z_k}
\addConstraint{z_k}{\leq z_{u,k}}
\addConstraint{\vec{r}_1}{= \vec{r}_0}
\addConstraint{\vec{r}_K}{= \vec{r}_f}
\addConstraint{\vec{v}_1}{= \vec{v}_0}
\addConstraint{\vec{v}_K}{= 0},
\end{mini!}

The design variables are defined as follows:
\begin{eqnarray*}
    \xi_k \triangleq \frac{\sigma_k}{m_k} & \vec{u}_k \triangleq \vec{T}_k & z_k \triangleq \ln{(m_k)}
\end{eqnarray*}

Here, for the $k$-th time step, $\vec{T}_k$ is the thrust vector, $m_k$ is the mass of the rocket, and $\sigma_k$ is the slack variable that corresponds to the norm of the thrust vector.

Static parameters such as wet-mass, dry-mass, and specific impulse were chosen to be representative of a rocket similar in size and performance to the first stage of the Falcon 9.
All of these quantities are listed below.
\begin{eqnarray*}
    I_{sp} = 282 \text{ s} & m_{wet} = 42.2 \cdot 10^3 \text{ kg} & m_{dry} = 22.2 \cdot 10^3 \text{ kg} \\
    \gamma_{gs} = 0.5 & g = 9.807 \text{ m/s\textsuperscript{2}} & \delta t = 0.1 \text{ s} \\
    t_f = 8 \text{ s} & F_{max} = 854 \text{ kN} & F_{min} = 0 \text{ kN}
\end{eqnarray*}

From these user defined parameters, the following were calculated.
\begin{eqnarray*}
    \alpha \triangleq \frac{1}{I_{sp} g} = 3.6 \cdot 10^{-4} \text{ s/m} & K \triangleq t_f/\delta t = 80
\end{eqnarray*}

The boundary conditions conditions for the 3-DOF are defined here.
\begin{align*}
    \vec{r}_0 &= \begin{bmatrix} 50 & 50 & 120 \end{bmatrix}^T \text{m} \\ 
    \vec{v}_0 &= \begin{bmatrix} -10 & 0 & -10 \end{bmatrix}^T \text{ m/s} \\ 
    \vec{r}_f &= \begin{bmatrix} 0 & 0 & 3 \end{bmatrix}^T \text{ m}
\end{align*}

The matrix referred to as $H_{gs}$, which is used to impose the glide-slope constraint is constructed as shown below.
\begin{equation*}
    H_{gs} = \begin{bmatrix}
        1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0
    \end{bmatrix}
\end{equation*}

In addition to the static parameters, there are parameters that change at each time step. 
First of which is the pointing constraint defined by $\gamma_{p,k}$, which linearly decreases from $\gamma_{p,1} = \pi/3$ rad to $\gamma_{p,K} = 0.001$ rad.
The lower and upper limits respectively set on the mass flow rate are computed using the following equations.
\begin{align*}
    z_{l,k} & = \ln{(m_{wet} - \alpha F_{max} \delta t k)} \\
    z_{u,k} & = \ln{(m_{wet} - \alpha F_{min} \delta t k)}
\end{align*}

From this definition, the term $\delta z_k = z_k - z_{l,k}$.
Lastly, the parameters that are involved in bounding the thrust ouput from the rocket are computed as shown.
\begin{align*}
    \mu_{min,k} & = F_{min} \exp{(-z_{l,k})} \\
    \mu_{max,k} & = F_{max} \exp{(-z_{l,k})}
\end{align*}

It is assumed that the attitude of the booster is deterministically related to the thrust vector direction, where the z-axis of the rocket body frame is anti-parallel always anti-parallel to the thrust direction. 
The camera pose is chosen such that at each timestep, the z-axis of the camera's body frame (``camera direction") is along the line-of-sight, which is a vector that connects the center of mass of the booster and the landing site.
These assumptions yield quaternion trajectories of the booster and camera attitude, respectively. 
Based on them, the histories of the angular velocity are also recovered in discrete timesteps as follows, using two quaternions $q_t$ and $q_{t+1}$.
%
\begin{align}
    \omega_t = \frac{2}{\Delta t}
    \begin{bmatrix}
        q_{t,0}q_{t+1,1} - q_{t,1}q_{t+1,0} - q_{t,2}q_{t+1,3} + q_{t,3}q_{t+1,2} \\
        q_{t,0}q_{t+1,2} + q_{t,1}q_{t+1,3} - q_{t,2}q_{t+1,0} - q_{t,3}q_{t+1,1} \\
        q_{t,0}q_{t+1,3} - q_{t,1}q_{t+1,2} + q_{t,2}q_{t+1,1} - q_{t,3}q_{t+1,0} \\
    \end{bmatrix}
\end{align}

Once we have the angular velocity, our control input can be simply derived as 
%
\begin{align}
    \Delta \omega_k = \omega_{k+1} - \omega_{k}
\end{align}

For the purposes of this analysis, no process noise has been injected onto the nominal trajectory. Therefore, the true state of the system is equivalent to the nominal trajectory solution.
Furthermore, we assume that the relative attitude between the camera and the booster is perfectly known \textit{a priori}; in reality, this directly relates to the camera's slew maneuver on the booster's body, where we expect much lower uncertainty compared to the dynamics and measurement of the booster during the landing. 

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.5\textwidth]{rocket_traj.png}
    \caption{Reference (True) Landing trajectory with camera attitude.}
    \label{fig:enter-label}
\end{figure}


\subsection{Simulation Environment}
Since it is not feasible to have a physical replica of a vehicle following a booster-like trajectory, a simulation environment is created in Blender to gather images from the perspective of the camera mounted on the booster. The simulated environment has an approximate scenery (marshy land around Boca Chica, Texas, USA), a landing pad and tower, along with a floating camera. An ArUco tag, described in Section \ref{sec:image_processing}, is placed at the bottom of the landing pad. This blender environment is shown in Fig. \ref{fig:blender_env}.

% insert figure here
\begin{figure}[htbp] \label{fig:blender_env}
    \centering
    \includegraphics[width=.3\textwidth]{blender_env.png}\hfill
    \caption{Blender environment used to gather sample camera data.}
\end{figure}

The Blender API allows us to command this floating camera to specific poses through a Python script. Using this API, the camera is moved along the camera-pose from the pre-determined trajectory timeseries from Section \ref{sec:dynamics}. An image is rendered at each of these poses. The entire batch of images is then passed into the vision based pose estimation pipeline described in Section \ref{sec:image_processing}. A selection of these images is shown in Fig. \ref{fig:blender_images}.

% insert figure here
\begin{figure}[htbp] \label{fig:blender_images}
    \includegraphics[width=.243\textwidth]{blender_sim/simulation_imgs/0.png}\hfill
    \includegraphics[width=.243\textwidth]{blender_sim/simulation_imgs/30.png}\hfill
    \includegraphics[width=.243\textwidth]{blender_sim/simulation_imgs/40.png}\hfill
    \includegraphics[width=.243\textwidth]{blender_sim/simulation_imgs/60.png}
    \caption{Camera outputs from the simulated environment in Blender at different points in the trajectory. The rendered image includes the ground surface, the landing tower, and .}
\end{figure}

Since camera-pose in the nominal trajectory is constrained to always face the marker at the landing pad, every rendered image also includes the marker. However, near the end of the trajectory, the field of view captures only a portion of the marker.

\subsection{Image Processing} \label{sec:image_processing}

To simplify the problem, the assumption is made that the landing pad will have a feature-rich ArUco marker, with known feature dimensions (for the real problem, this will just be extended to utilize the 3-dimensional features of the landing pad itself). 
The generated image of the environment (that includes the ArUco marker) at each time step is used to extract the camera's pose. 
With the fully defined camera intrinsic properties and the perfect knowledge of the location of ArUco features, extracting the pose is reduced to solving the camera extrinsics problem. 
The OpenCV library is used for the feature detection and matching algorithm for this implementation.
To briefly discuss the specifics of the OpenCV library's functions, first we used a built in function to detect the ArUco marker's corners and local coordinate frame. 
We confirm that the marker detection was done correctly by plotting the corners and axes on each image. 
Figure \ref{fig:marker_} illustrates the detected ArUco marker for a specific time step in our rocket's trajectory. OpenCV was used to generate this specific marker; in a case where multiple markers are necessary for pose estimation, we would generate different ArUco markers so the detection function would be able to produce a pose estimate for each marker that is clearly captured in an image.

% insert figure here
\begin{figure}[ht!] 
    \centerline{\includegraphics[width=0.4\textwidth]{detectedMarker.png}}
    \caption{Detected ArUco Marker with Defined Corners and Coordinate Frame.}
    \label{fig:marker_}
\end{figure}
% insert figure here

With camera pose now known relative to the world frame as well as relative to the body frame of the rocket, we can extract the rocket pose. 
The camera intrinsic matrix was generated using defined parameters from the blender simulation.

\[
K = \begin{bmatrix}
424.3378 & 0 & 424.0 \\
0 & 424.3378 & 240.0 \\
0 & 0 & 1 \\
\end{bmatrix}
\]

With the ArUco marker simulated with a 10 meter side length in Blender, the origin of the marker's frame as well as its coordinates are known in 3D space. 
The 2D pixel coordinates of the origin and corners are found by OpenCV's detection function. 
The 3D known coordinates coupled with the 2D pixel coordinates allow the PnP problem to be solved with OpenCV's pose estimation function, yielding the camera pose.
After pre-processing the image, the first step of the PnP problem is to project the 3D points onto the 2D image plane using the camera intrinsics and an initial estimate of the rotation and translation of the camera relative to the marker. 
Since we did not define an initial estimate for the camera pose, OpenCV automatically defines this for us, although the specifics of this estimate are unknown.
An optimization algorithm, likely Levenberg-Marquadt, iteratively reduces the projection error until a convergence criteria is satisfied. 
The algorithm then returns the final translation and rotation, which can generally be trusted as an accurate pose estimate when using ArUco markers. 
The resulting pose is then converted to OpenCV convention and transformed into the world frame to account for the landing pad offset from the actual ground.
Image \ref{fig:CV_convention} illustrates the transformation to the OpenCV frame convention.

% insert figure here
\begin{figure}[ht!] 
    \centerline{\includegraphics[width=0.45\textwidth]{CVtoGL.png}}
    \caption{OpenGL to OpenCV Frame Convention \cite{openCV2024web}.}
    \label{fig:CV_convention}
\end{figure}
% insert figure here

Now, with the camera pose correctly defined in the world frame, we convert the rotation matrix to a quaternion representation to represent the attitude and avoid any potential singularities. 


\subsection{Attitude filtering}

The filtering of the rotational motion is executed by a multiplicative extended Kalman filter (MEKF) \cite{markley2003mekf} by choosing a state vector composed of pose error (i.e., error Modified Rodrigues Parameters (MRPs)) and angular velocity in a body frame.
As an error MRPs do not have a unitary norm constraint like a quaternion, computation of the covariance matrix in MRP space simplifies the computation. 
The kinematics of the error MRPs are linearized along the reference quaternion history (i.e., $\delta \boldsymbol{p}_{t|t} = 0$), where the a posteriori estimate of the error MRP is then used to update the reference quaternion. 
% ($\boldsymbol{q}_{\text{ref}}^{k+1}(t) = \delta \boldsymbol{q}(\delta \boldsymbol{p}^{+}(t)) \circ \boldsymbol{q}_{\text{ref}}^k(t)$). 

The state that variable described above can be written as follows:
\begin{align*}
    \mu = \begin{bmatrix}
        \mu^p \\
        \mu^{\omega}
    \end{bmatrix}
\end{align*}
    
The prediction step is expressed as follows:
%
\begin{align*}
    q_{t+1|t} & = f(q_{t|t}, \omega_{t|t}, u_t) \\
    \mu_{t+1|t}
     & = A_{t} 
    \mu_{t|t} + B_t u_t  \\
    \Sigma_{t+1|t} & = A_t \Sigma_{t|t} A_t^T + Q_t,
\end{align*}

The linearized discrete-time state transition matrix and control input matrix are derived from the continuous-time linearized dynamics: 
%
\begin{align}
    \dot{x} = 
    \begin{bmatrix}
        -\frac{1}{2} [\omega_t]^{\times} & \mathbb{I}_3 \\
        0 & 0 
    \end{bmatrix} x =: \Lambda_t x, 
    \quad 
    x :=  \begin{bmatrix}
        \delta p \\ \omega
    \end{bmatrix},
\end{align}
%
where $[\omega_t]^{\times}$ denotes a skew matrix of a vector $\omega_t$.
By taking the state transition matrix, we obtain the discrete-time state transition matrix 
%
\begin{align}
    A_t & = e^{\Lambda_t \Delta t} = \begin{bmatrix}
        -e ^{-\frac{1}{2}[\omega]^{\times}\Delta t} & 
        \int_{0}^{\Delta t} e^{-\frac{1}{2}[\omega]^{\times} \tau d\tau } \\
        0 & \mathbb{I}_3
    \end{bmatrix}, \\
    B_t & = \begin{bmatrix}
        0 \\ \mathbb{I}_3
    \end{bmatrix}
\end{align}
%
The efficient computation of the matrix $A_t$ is elaborated in Ref. \citenum{tweddle2015relative}. 
Note that we are assuming the impulsive control $u_t = \Delta \omega_t$ in this paper, which leads to the definition of the discrete-time control input matrix $B_t$.

The update step is expressed as follows:
%
\begin{align*}
    K_t & =  \Sigma_{t+1|t} C_t^\top (C_t \Sigma_{t+1|t} C_t^T + R_t)^{-1} \\
    \mu_{t+1|t+1}
     & = 
    \mu_{t+1|t} + K_t \left(y_t - g\left(\mu_{t+1|t} \right) \right) 
     \\
     \Sigma_{t+1|t+1} & = \Sigma_{t+1|t} - K_t C_t  \Sigma_{t+1|t}  \\
\end{align*}
Following the update step, the reset step is performed. 
During this step, the reference quaternion is updated using posterior mean from the update step, and the component of the posterior mean that corresponds to the error MRP is reset to zero. 
This is shown below.
\begin{align*}
    \delta p_{t} & = \mu_{t+1|t+1}^{p}\\
    q_{t+1|t+1} & = \delta q(\delta p_{t}) \circ q_{t+1|t} \\
    \mu_{t+1|t+1} & = \begin{bmatrix} 0 \\
    \mu_{t+1|t+1}^{\omega}\end{bmatrix}
\end{align*}

The measurement model $g(\mu)$ is quite simple for this problem formulation. 
The IMU measurements consist of an estimate of the absolute attitude itself along with the angular velocity. 
The estimate of the absolute attitude output by the IMU corresponds to error between the the ground truth attitude and the reference quaternion in the filter (i.e. $q_{t+1|t}$) expressed as an error MRP to match the form of the state. 
Thus the measurement model for the IMU outputs is as follows.
\begin{equation*}
    g(\mu_{t+1|t})^{IMU} = C_t^{IMU} \mu_{t+1|t} = \begin{bmatrix}
        I & 0 \\
        0 & I
    \end{bmatrix} \mu_{t+1|t}
\end{equation*}

The output that comes directly from the camera pose measurements is the estimate of the ground truth pose. 
This is converted to an error between the estimate and the reference quaternion, again in order to match the form of the state. 
Thus the measurement model for this component of the measurement space is represented as follows.
\begin{equation*}
    g(\mu_{t+1|t})^{c} = C_t^c \mu_{t+1|t} = I \mu_{t+1|t}
\end{equation*}

It follows that the overall measurement model can be constructed as shown below.
\begin{equation*}
    g(\mu_{t+1|t} = \begin{bmatrix}
        g(\mu_{t+1|t})^{c} \\ g(\mu_{t+1|t})^{IMU}
    \end{bmatrix} = C_t \mu_{t+1|t} = \begin{bmatrix}
        I & 0 \\ I & 0 \\ 0 & I
    \end{bmatrix} \mu_{t+1|t}
\end{equation*}

For analysis of the MEKF, actual artificial measurements had to also be generated. Firstly for the IMU, this was done using the following expression, which leverages the fact that the expected value of $\mu_{t+1|t}$ is 0.
\begin{equation*}
    y_t^{IMU} = \begin{bmatrix}
         0 \\ \omega_t
    \end{bmatrix} + V_t
\end{equation*}
Where $\omega_t$ is the true angular velocity, $V_t^{IMU}$ is uncorrelated random noise with covariance $R_t^{IMU}$.

The camera measurements were generated a bit more subtly. The quaternion output from the camera model is slightly perturbed by some $\delta q$ to emulate noise as shown here.
\begin{equation*}
    \hat{q}^c = \delta q \circ q^c
\end{equation*}

Then the perturbed quaternion is converted to an error MRP for the final measurement generation.
\begin{equation*}
    y_t^c = \hat{q}^c \circ q_{t+1|t}^*
\end{equation*}
Where $q_{t+1|t}^*$ is the conjugate of the reference quaternion after it is propagated forward in time.

Thus the full measurement vector at each time step can be expressed as follows.
\begin{equation*}
    y_t = \begin{bmatrix} y_t^c \\ y_t^{IMU} \end{bmatrix} = \begin{bmatrix}
        \hat{q}^c \circ q_{t+1|t}^* \\ 0 \\ \omega_t
    \end{bmatrix} + \begin{bmatrix}
        0 \\ V_t
    \end{bmatrix}
\end{equation*}


\subsection{Result Validation and Sensor Fusion}
To validate the camera-based pose estimate and determine whether it is a viable method for pose estimation for the booster landing problem, the visual estimate is benchmarked against inertial navigation, i.e. pose estimation using data from an Inertial Measurement Unit. 
As inertial navigation is the current standard method for pose estimation on launch vehicles, it would be useful to validate whether fusing the vision-based pose estimate with the inertial pose estimate provides an improved final result. 


\section{Results}

\subsection{Camera Pose Estimation Results}
After solving the PnP problem and transforming to the correct frame, we compared the ground truth time history of the position and attitude to our vision estimation results.

% insert figure here
\begin{figure}[ht!] 
    \centerline{\includegraphics[width=0.5\textwidth]{translation_comp.png}}
    \caption{Ground Truth versus Estimated Position.}
    \label{fig:trans_comp}
\end{figure}
% insert figure here

% insert figure here
\begin{figure}[ht!] 
    \centerline{\includegraphics[width=0.5\textwidth]{quaternion_comp.png}}
    \caption{Ground Truth versus Estimated Attitude.}
    \label{fig:quat_comp}
\end{figure}
% insert figure here

As illustrated in Fig. \ref{fig:trans_comp} and \ref{fig:quat_comp}, the pose estimation of the camera from the PnP problem tracks well; the estimated position and attitude time histories match the ground truth. 
It should be noted that the 'spikes' in the plots can partially be attributed to the ArUco marker not being detected at every time step.
Potential solutions to this issue are discussed in the conclusion of this paper.


\subsection{Vision Denied Case}

% insert figure here
\begin{figure}[ht!] 
    \centerline{\includegraphics[width=0.3\textwidth]{omega_result_plot_vision_denied.png}}
    \caption{Rotation rate results in the vision denied case.}
    \label{fig:results_novision}
\end{figure}

\begin{figure}[ht!] 
    \centerline{\includegraphics[width=0.3\textwidth]{mrp_result_plot_vision_denied.png}}
    \caption{Modified rodrigues parameter error results in the vision denied case.}
    \label{fig:results_novision}
\end{figure}
% insert figure here

\subsection{Vision Enhanced Case}


% insert figure here
\begin{figure}[ht!] 
    \centerline{\includegraphics[width=0.3\textwidth]{omega_result_plot_vision_enhanced.png}}
    \caption{Rotation rate results in the vision enhanced case.}
    \label{fig:results_novision}
\end{figure}

\begin{figure}[ht!] 
    \centerline{\includegraphics[width=0.3\textwidth]{mrp_result_plot_vision_enhanced.png}}
    \caption{Modified rodrigues parameter error results in the vision enhanced case.}
    \label{fig:results_novision}
\end{figure}
% insert figure here



\section{Conclusion}

As mentioned previously, over the course of the rocket's trajectory, not every time step had an image where the ArUco marker could be detected by OpenCV.
This happened most often at the beginning and end of the trajectory, where the ArUco tag was either too far away to be detected, or too close where not all four corners of the tag were captured by the Blender image. 
In a real-world scenario, having multiple cameras and/or multiple ArUco markers would solve this issue, enabling us to generate vision-based pose estimation at every time step.
Additionally, terrain relative navigation is a viable alternative to ArUco marker detection, especially when the rocket is too far away from the landing pad for a camera to capture high-resolution images. 
This would also allow us to avoid ever relying on solely the IMU measurements. 
Since our comparison of the results of the vision-denied case to the vision-enhanced case showed that adding vision significantly improved the MRP error, having vision based pose estimates at every time step would be very beneficial to achieving the centimeter-level accuracy required for catching the booster upon landing.

% \section*{Acknowledgment}




\bibliographystyle{ieeetr}
\bibliography{reference}

\end{document}
